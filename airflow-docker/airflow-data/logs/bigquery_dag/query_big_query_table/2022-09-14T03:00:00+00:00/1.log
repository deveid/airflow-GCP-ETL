[2022-09-17 01:35:00,660] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-17 01:35:00,728] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-17 01:35:00,731] {taskinstance.py:1017} INFO - 
--------------------------------------------------------------------------------
[2022-09-17 01:35:00,733] {taskinstance.py:1018} INFO - Starting attempt 1 of 3
[2022-09-17 01:35:00,734] {taskinstance.py:1019} INFO - 
--------------------------------------------------------------------------------
[2022-09-17 01:35:00,781] {taskinstance.py:1038} INFO - Executing <Task(BigQueryOperator): query_big_query_table> on 2022-09-14T03:00:00+00:00
[2022-09-17 01:35:00,797] {standard_task_runner.py:51} INFO - Started process 50259 to run task
[2022-09-17 01:35:00,811] {standard_task_runner.py:75} INFO - Running: ['airflow', 'tasks', 'run', 'bigquery_dag', 'query_big_query_table', '2022-09-14T03:00:00+00:00', '--job-id', '142', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_bigquery.py', '--cfg-path', '/tmp/tmp9vovxvvn']
[2022-09-17 01:35:00,821] {standard_task_runner.py:76} INFO - Job 142: Subtask query_big_query_table
[2022-09-17 01:35:00,963] {logging_mixin.py:103} INFO - Running <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [running]> on host 27377494ab8b
[2022-09-17 01:35:01,091] {taskinstance.py:1232} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Dave
AIRFLOW_CTX_DAG_ID=bigquery_dag
AIRFLOW_CTX_TASK_ID=query_big_query_table
AIRFLOW_CTX_EXECUTION_DATE=2022-09-14T03:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-09-14T03:00:00+00:00
[2022-09-17 01:35:01,099] {bigquery.py:680} INFO - Executing: 
           select id from githubarchive.day.20220914
        
[2022-09-17 01:35:01,161] {logging_mixin.py:103} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-09-17 01:35:01,168] {taskinstance.py:1396} ERROR - [Errno 2] No such file or directory: '/airflow-data/support/keys/quick-keel-213611-3a513d19a935.json'
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1086, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1260, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1300, in _execute_task
    result = task_copy.execute(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 706, in execute
    encryption_configuration=self.encryption_configuration,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py", line 2054, in run_query
    if not self.project_id:
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/common/hooks/base_google.py", line 307, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/common/hooks/base_google.py", line 237, in _get_credentials_and_project_id
    delegates=delegates,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 309, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 238, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 286, in _get_credentials_using_key_path
    self.key_path, scopes=self.scopes
  File "/home/airflow/.local/lib/python3.6/site-packages/google/oauth2/service_account.py", line 234, in from_service_account_file
    filename, require=["client_email", "token_uri"]
  File "/home/airflow/.local/lib/python3.6/site-packages/google/auth/_service_account_info.py", line 72, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/airflow-data/support/keys/quick-keel-213611-3a513d19a935.json'
[2022-09-17 01:35:01,192] {taskinstance.py:1440} INFO - Marking task as UP_FOR_RETRY. dag_id=bigquery_dag, task_id=query_big_query_table, execution_date=20220914T030000, start_date=20220917T013500, end_date=20220917T013501
[2022-09-17 01:35:01,270] {local_task_job.py:118} INFO - Task exited with return code 1
[2022-09-17 01:37:37,261] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-17 01:37:37,419] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-17 01:37:37,420] {taskinstance.py:1017} INFO - 
--------------------------------------------------------------------------------
[2022-09-17 01:37:37,422] {taskinstance.py:1018} INFO - Starting attempt 1 of 3
[2022-09-17 01:37:37,424] {taskinstance.py:1019} INFO - 
--------------------------------------------------------------------------------
[2022-09-17 01:37:37,510] {taskinstance.py:1038} INFO - Executing <Task(BigQueryOperator): query_big_query_table> on 2022-09-14T03:00:00+00:00
[2022-09-17 01:37:37,534] {standard_task_runner.py:51} INFO - Started process 1506 to run task
[2022-09-17 01:37:37,558] {standard_task_runner.py:75} INFO - Running: ['airflow', 'tasks', 'run', 'bigquery_dag', 'query_big_query_table', '2022-09-14T03:00:00+00:00', '--job-id', '161', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_bigquery.py', '--cfg-path', '/tmp/tmpu8v5shc6']
[2022-09-17 01:37:37,575] {standard_task_runner.py:76} INFO - Job 161: Subtask query_big_query_table
[2022-09-17 01:37:37,874] {logging_mixin.py:103} INFO - Running <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [running]> on host 27377494ab8b
[2022-09-17 01:37:38,111] {taskinstance.py:1232} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Dave
AIRFLOW_CTX_DAG_ID=bigquery_dag
AIRFLOW_CTX_TASK_ID=query_big_query_table
AIRFLOW_CTX_EXECUTION_DATE=2022-09-14T03:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-09-14T03:00:00+00:00
[2022-09-17 01:37:38,128] {bigquery.py:680} INFO - Executing: 
           select id from githubarchive.day.20220914
        
[2022-09-17 01:37:38,244] {logging_mixin.py:103} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-09-17 01:37:38,248] {taskinstance.py:1396} ERROR - [Errno 2] No such file or directory: '/Users/david/Documents/airflow_pr/airflow-docker/airflow-data/support/keys/quick-keel-213611-3a513d19a935.json'
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1086, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1260, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1300, in _execute_task
    result = task_copy.execute(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 706, in execute
    encryption_configuration=self.encryption_configuration,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py", line 2054, in run_query
    if not self.project_id:
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/common/hooks/base_google.py", line 307, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/common/hooks/base_google.py", line 237, in _get_credentials_and_project_id
    delegates=delegates,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 309, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 238, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 286, in _get_credentials_using_key_path
    self.key_path, scopes=self.scopes
  File "/home/airflow/.local/lib/python3.6/site-packages/google/oauth2/service_account.py", line 234, in from_service_account_file
    filename, require=["client_email", "token_uri"]
  File "/home/airflow/.local/lib/python3.6/site-packages/google/auth/_service_account_info.py", line 72, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/Users/david/Documents/airflow_pr/airflow-docker/airflow-data/support/keys/quick-keel-213611-3a513d19a935.json'
[2022-09-17 01:37:38,283] {taskinstance.py:1440} INFO - Marking task as UP_FOR_RETRY. dag_id=bigquery_dag, task_id=query_big_query_table, execution_date=20220914T030000, start_date=20220917T013737, end_date=20220917T013738
[2022-09-17 01:37:38,432] {local_task_job.py:118} INFO - Task exited with return code 1
[2022-09-17 01:41:01,036] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-17 01:41:01,156] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-17 01:41:01,159] {taskinstance.py:1017} INFO - 
--------------------------------------------------------------------------------
[2022-09-17 01:41:01,160] {taskinstance.py:1018} INFO - Starting attempt 1 of 3
[2022-09-17 01:41:01,162] {taskinstance.py:1019} INFO - 
--------------------------------------------------------------------------------
[2022-09-17 01:41:01,247] {taskinstance.py:1038} INFO - Executing <Task(BigQueryOperator): query_big_query_table> on 2022-09-14T03:00:00+00:00
[2022-09-17 01:41:01,258] {standard_task_runner.py:51} INFO - Started process 3385 to run task
[2022-09-17 01:41:01,295] {standard_task_runner.py:75} INFO - Running: ['airflow', 'tasks', 'run', 'bigquery_dag', 'query_big_query_table', '2022-09-14T03:00:00+00:00', '--job-id', '183', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_bigquery.py', '--cfg-path', '/tmp/tmp0z9kackj']
[2022-09-17 01:41:01,329] {standard_task_runner.py:76} INFO - Job 183: Subtask query_big_query_table
[2022-09-17 01:41:01,643] {logging_mixin.py:103} INFO - Running <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [running]> on host 27377494ab8b
[2022-09-17 01:41:01,964] {taskinstance.py:1232} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Dave
AIRFLOW_CTX_DAG_ID=bigquery_dag
AIRFLOW_CTX_TASK_ID=query_big_query_table
AIRFLOW_CTX_EXECUTION_DATE=2022-09-14T03:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-09-14T03:00:00+00:00
[2022-09-17 01:41:01,980] {bigquery.py:680} INFO - Executing: 
           select id from githubarchive.day.20220914
        
[2022-09-17 01:41:02,082] {logging_mixin.py:103} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-09-17 01:41:02,090] {taskinstance.py:1396} ERROR - [Errno 2] No such file or directory: '/opt/airflow/support/keys/quick-keel-213611-3a513d19a935.json'
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1086, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1260, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1300, in _execute_task
    result = task_copy.execute(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 706, in execute
    encryption_configuration=self.encryption_configuration,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py", line 2054, in run_query
    if not self.project_id:
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/common/hooks/base_google.py", line 307, in project_id
    _, project_id = self._get_credentials_and_project_id()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/common/hooks/base_google.py", line 237, in _get_credentials_and_project_id
    delegates=delegates,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 309, in get_credentials_and_project_id
    return _CredentialProvider(*args, **kwargs).get_credentials_and_project()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 238, in get_credentials_and_project
    credentials, project_id = self._get_credentials_using_key_path()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 286, in _get_credentials_using_key_path
    self.key_path, scopes=self.scopes
  File "/home/airflow/.local/lib/python3.6/site-packages/google/oauth2/service_account.py", line 234, in from_service_account_file
    filename, require=["client_email", "token_uri"]
  File "/home/airflow/.local/lib/python3.6/site-packages/google/auth/_service_account_info.py", line 72, in from_filename
    with io.open(filename, "r", encoding="utf-8") as json_file:
FileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/support/keys/quick-keel-213611-3a513d19a935.json'
[2022-09-17 01:41:02,127] {taskinstance.py:1440} INFO - Marking task as UP_FOR_RETRY. dag_id=bigquery_dag, task_id=query_big_query_table, execution_date=20220914T030000, start_date=20220917T014101, end_date=20220917T014102
[2022-09-17 01:41:02,321] {local_task_job.py:118} INFO - Task exited with return code 1
[2022-09-17 01:45:37,148] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-17 01:45:37,319] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-17 01:45:37,326] {taskinstance.py:1017} INFO - 
--------------------------------------------------------------------------------
[2022-09-17 01:45:37,331] {taskinstance.py:1018} INFO - Starting attempt 1 of 3
[2022-09-17 01:45:37,333] {taskinstance.py:1019} INFO - 
--------------------------------------------------------------------------------
[2022-09-17 01:45:37,407] {taskinstance.py:1038} INFO - Executing <Task(BigQueryOperator): query_big_query_table> on 2022-09-14T03:00:00+00:00
[2022-09-17 01:45:37,420] {standard_task_runner.py:51} INFO - Started process 6649 to run task
[2022-09-17 01:45:37,465] {standard_task_runner.py:75} INFO - Running: ['airflow', 'tasks', 'run', 'bigquery_dag', 'query_big_query_table', '2022-09-14T03:00:00+00:00', '--job-id', '203', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_bigquery.py', '--cfg-path', '/tmp/tmp1l8153vu']
[2022-09-17 01:45:37,491] {standard_task_runner.py:76} INFO - Job 203: Subtask query_big_query_table
[2022-09-17 01:45:37,828] {logging_mixin.py:103} INFO - Running <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [running]> on host 27377494ab8b
[2022-09-17 01:45:38,102] {taskinstance.py:1232} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Dave
AIRFLOW_CTX_DAG_ID=bigquery_dag
AIRFLOW_CTX_TASK_ID=query_big_query_table
AIRFLOW_CTX_EXECUTION_DATE=2022-09-14T03:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-09-14T03:00:00+00:00
[2022-09-17 01:45:38,118] {bigquery.py:680} INFO - Executing: 
           select id from githubarchive.day.20220914
        
[2022-09-17 01:45:38,250] {logging_mixin.py:103} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-09-17 01:45:38,356] {bigquery.py:1510} INFO - Inserting job airflow_1663379138354205_06d2bf8b43be9b79a3e89edf3b53e367
[2022-09-17 01:45:44,184] {taskinstance.py:1142} INFO - Marking task as SUCCESS. dag_id=bigquery_dag, task_id=query_big_query_table, execution_date=20220914T030000, start_date=20220917T014537, end_date=20220917T014544
[2022-09-17 01:45:44,531] {taskinstance.py:1195} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-09-17 01:45:44,562] {local_task_job.py:118} INFO - Task exited with return code 0
[2022-09-18 18:33:13,910] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-18 18:33:14,081] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-18 18:33:14,085] {taskinstance.py:1017} INFO - 
--------------------------------------------------------------------------------
[2022-09-18 18:33:14,088] {taskinstance.py:1018} INFO - Starting attempt 1 of 3
[2022-09-18 18:33:14,090] {taskinstance.py:1019} INFO - 
--------------------------------------------------------------------------------
[2022-09-18 18:33:14,233] {taskinstance.py:1038} INFO - Executing <Task(BigQueryOperator): query_big_query_table> on 2022-09-14T03:00:00+00:00
[2022-09-18 18:33:14,340] {standard_task_runner.py:51} INFO - Started process 2088 to run task
[2022-09-18 18:33:14,482] {standard_task_runner.py:75} INFO - Running: ['airflow', 'tasks', 'run', 'bigquery_dag', 'query_big_query_table', '2022-09-14T03:00:00+00:00', '--job-id', '282', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_bigquery.py', '--cfg-path', '/tmp/tmpev4a3dk4']
[2022-09-18 18:33:15,567] {standard_task_runner.py:76} INFO - Job 282: Subtask query_big_query_table
[2022-09-18 18:33:16,449] {logging_mixin.py:103} INFO - Running <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [running]> on host 27377494ab8b
[2022-09-18 18:33:17,229] {taskinstance.py:1232} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Dave
AIRFLOW_CTX_DAG_ID=bigquery_dag
AIRFLOW_CTX_TASK_ID=query_big_query_table
AIRFLOW_CTX_EXECUTION_DATE=2022-09-14T03:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-09-14T03:00:00+00:00
[2022-09-18 18:33:17,308] {bigquery.py:680} INFO - Executing: 
           select id from githubarchive.day.20220914
        
[2022-09-18 18:33:17,951] {logging_mixin.py:103} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-09-18 18:33:18,372] {bigquery.py:1510} INFO - Inserting job airflow_1663525998360247_06d2bf8b43be9b79a3e89edf3b53e367
[2022-09-18 18:33:25,110] {taskinstance.py:1142} INFO - Marking task as SUCCESS. dag_id=bigquery_dag, task_id=query_big_query_table, execution_date=20220914T030000, start_date=20220918T183313, end_date=20220918T183325
[2022-09-18 18:33:29,195] {local_task_job.py:170} WARNING - State of this instance has been externally set to success. Terminating instance.
[2022-09-18 18:33:29,404] {process_utils.py:95} INFO - Sending Signals.SIGTERM to GPID 2088
[2022-09-18 18:33:29,419] {taskinstance.py:1214} ERROR - Received SIGTERM. Terminating subprocesses.
[2022-09-18 18:33:29,428] {bigquery.py:740} INFO - Cancelling running query
[2022-09-18 18:33:29,461] {logging_mixin.py:103} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:1371 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.cancel_job`.
[2022-09-18 18:33:30,422] {bigquery.py:1398} INFO - No running BigQuery jobs to cancel.
[2022-09-18 18:33:30,753] {process_utils.py:61} INFO - Process psutil.Process(pid=2088, status='terminated', exitcode=1, started='18:33:13') (2088) terminated with exit code 1
[2022-09-18 18:33:30,766] {local_task_job.py:118} INFO - Task exited with return code 1
[2022-09-18 18:39:48,032] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-18 18:39:48,128] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-18 18:39:48,130] {taskinstance.py:1017} INFO - 
--------------------------------------------------------------------------------
[2022-09-18 18:39:48,132] {taskinstance.py:1018} INFO - Starting attempt 1 of 3
[2022-09-18 18:39:48,134] {taskinstance.py:1019} INFO - 
--------------------------------------------------------------------------------
[2022-09-18 18:39:48,213] {taskinstance.py:1038} INFO - Executing <Task(BigQueryOperator): query_big_query_table> on 2022-09-14T03:00:00+00:00
[2022-09-18 18:39:48,232] {standard_task_runner.py:51} INFO - Started process 714 to run task
[2022-09-18 18:39:48,278] {standard_task_runner.py:75} INFO - Running: ['airflow', 'tasks', 'run', 'bigquery_dag', 'query_big_query_table', '2022-09-14T03:00:00+00:00', '--job-id', '310', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_bigquery.py', '--cfg-path', '/tmp/tmpxjhp37_3']
[2022-09-18 18:39:48,292] {standard_task_runner.py:76} INFO - Job 310: Subtask query_big_query_table
[2022-09-18 18:39:48,448] {logging_mixin.py:103} INFO - Running <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [running]> on host 27377494ab8b
[2022-09-18 18:39:48,697] {taskinstance.py:1232} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Dave
AIRFLOW_CTX_DAG_ID=bigquery_dag
AIRFLOW_CTX_TASK_ID=query_big_query_table
AIRFLOW_CTX_EXECUTION_DATE=2022-09-14T03:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-09-14T03:00:00+00:00
[2022-09-18 18:39:48,714] {bigquery.py:680} INFO - Executing: 
           select id from githubarchive.day.20220914
        
[2022-09-18 18:39:48,812] {logging_mixin.py:103} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-09-18 18:39:48,988] {bigquery.py:1510} INFO - Inserting job airflow_1663526388960105_06d2bf8b43be9b79a3e89edf3b53e367
[2022-09-18 18:39:51,047] {taskinstance.py:1142} INFO - Marking task as SUCCESS. dag_id=bigquery_dag, task_id=query_big_query_table, execution_date=20220914T030000, start_date=20220918T183948, end_date=20220918T183951
[2022-09-18 18:39:51,972] {taskinstance.py:1195} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-09-18 18:39:52,083] {local_task_job.py:118} INFO - Task exited with return code 0
[2022-09-18 19:36:07,652] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-18 19:36:07,772] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-18 19:36:07,775] {taskinstance.py:1017} INFO - 
--------------------------------------------------------------------------------
[2022-09-18 19:36:07,777] {taskinstance.py:1018} INFO - Starting attempt 1 of 3
[2022-09-18 19:36:07,779] {taskinstance.py:1019} INFO - 
--------------------------------------------------------------------------------
[2022-09-18 19:36:07,880] {taskinstance.py:1038} INFO - Executing <Task(BigQueryOperator): query_big_query_table> on 2022-09-14T03:00:00+00:00
[2022-09-18 19:36:07,903] {standard_task_runner.py:51} INFO - Started process 689 to run task
[2022-09-18 19:36:07,931] {standard_task_runner.py:75} INFO - Running: ['airflow', 'tasks', 'run', 'bigquery_dag', 'query_big_query_table', '2022-09-14T03:00:00+00:00', '--job-id', '397', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_bigquery.py', '--cfg-path', '/tmp/tmpyke500_2']
[2022-09-18 19:36:07,952] {standard_task_runner.py:76} INFO - Job 397: Subtask query_big_query_table
[2022-09-18 19:36:08,267] {logging_mixin.py:103} INFO - Running <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [running]> on host 27377494ab8b
[2022-09-18 19:36:08,575] {taskinstance.py:1232} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Dave
AIRFLOW_CTX_DAG_ID=bigquery_dag
AIRFLOW_CTX_TASK_ID=query_big_query_table
AIRFLOW_CTX_EXECUTION_DATE=2022-09-14T03:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-09-14T03:00:00+00:00
[2022-09-18 19:36:08,601] {bigquery.py:680} INFO - Executing: 
           select id from githubarchive.day.20220914
        
[2022-09-18 19:36:08,688] {logging_mixin.py:103} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-09-18 19:36:08,870] {bigquery.py:1510} INFO - Inserting job airflow_1663529768851114_06d2bf8b43be9b79a3e89edf3b53e367
[2022-09-18 19:36:10,684] {taskinstance.py:1142} INFO - Marking task as SUCCESS. dag_id=bigquery_dag, task_id=query_big_query_table, execution_date=20220914T030000, start_date=20220918T193607, end_date=20220918T193610
[2022-09-18 19:36:11,314] {taskinstance.py:1195} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-09-18 19:36:11,423] {local_task_job.py:118} INFO - Task exited with return code 0
[2022-09-18 19:40:29,640] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-18 19:40:29,678] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-18 19:40:29,680] {taskinstance.py:1017} INFO - 
--------------------------------------------------------------------------------
[2022-09-18 19:40:29,681] {taskinstance.py:1018} INFO - Starting attempt 1 of 3
[2022-09-18 19:40:29,682] {taskinstance.py:1019} INFO - 
--------------------------------------------------------------------------------
[2022-09-18 19:40:29,724] {taskinstance.py:1038} INFO - Executing <Task(BigQueryOperator): query_big_query_table> on 2022-09-14T03:00:00+00:00
[2022-09-18 19:40:29,744] {standard_task_runner.py:51} INFO - Started process 3103 to run task
[2022-09-18 19:40:29,755] {standard_task_runner.py:75} INFO - Running: ['airflow', 'tasks', 'run', 'bigquery_dag', 'query_big_query_table', '2022-09-14T03:00:00+00:00', '--job-id', '422', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_bigquery.py', '--cfg-path', '/tmp/tmpzytsfeub']
[2022-09-18 19:40:29,771] {standard_task_runner.py:76} INFO - Job 422: Subtask query_big_query_table
[2022-09-18 19:40:29,959] {logging_mixin.py:103} INFO - Running <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [running]> on host 27377494ab8b
[2022-09-18 19:40:30,114] {taskinstance.py:1232} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Dave
AIRFLOW_CTX_DAG_ID=bigquery_dag
AIRFLOW_CTX_TASK_ID=query_big_query_table
AIRFLOW_CTX_EXECUTION_DATE=2022-09-14T03:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-09-14T03:00:00+00:00
[2022-09-18 19:40:30,125] {bigquery.py:680} INFO - Executing: 
           select id from githubarchive.day.20220914
        
[2022-09-18 19:40:30,234] {logging_mixin.py:103} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-09-18 19:40:30,370] {bigquery.py:1510} INFO - Inserting job airflow_1663530030363844_06d2bf8b43be9b79a3e89edf3b53e367
[2022-09-18 19:40:32,025] {taskinstance.py:1142} INFO - Marking task as SUCCESS. dag_id=bigquery_dag, task_id=query_big_query_table, execution_date=20220914T030000, start_date=20220918T194029, end_date=20220918T194032
[2022-09-18 19:40:32,523] {taskinstance.py:1195} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-09-18 19:40:32,603] {local_task_job.py:118} INFO - Task exited with return code 0
[2022-09-18 20:15:57,291] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-18 20:15:57,832] {taskinstance.py:826} INFO - Dependencies all met for <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [queued]>
[2022-09-18 20:15:57,847] {taskinstance.py:1017} INFO - 
--------------------------------------------------------------------------------
[2022-09-18 20:15:57,854] {taskinstance.py:1018} INFO - Starting attempt 1 of 3
[2022-09-18 20:15:57,875] {taskinstance.py:1019} INFO - 
--------------------------------------------------------------------------------
[2022-09-18 20:15:58,229] {taskinstance.py:1038} INFO - Executing <Task(BigQueryOperator): query_big_query_table> on 2022-09-14T03:00:00+00:00
[2022-09-18 20:15:58,365] {standard_task_runner.py:51} INFO - Started process 1396 to run task
[2022-09-18 20:15:58,478] {standard_task_runner.py:75} INFO - Running: ['airflow', 'tasks', 'run', 'bigquery_dag', 'query_big_query_table', '2022-09-14T03:00:00+00:00', '--job-id', '470', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/dag_with_bigquery.py', '--cfg-path', '/tmp/tmpcqy5_bxr']
[2022-09-18 20:15:58,569] {standard_task_runner.py:76} INFO - Job 470: Subtask query_big_query_table
[2022-09-18 20:15:59,684] {logging_mixin.py:103} INFO - Running <TaskInstance: bigquery_dag.query_big_query_table 2022-09-14T03:00:00+00:00 [running]> on host 27377494ab8b
[2022-09-18 20:16:00,473] {taskinstance.py:1232} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Dave
AIRFLOW_CTX_DAG_ID=bigquery_dag
AIRFLOW_CTX_TASK_ID=query_big_query_table
AIRFLOW_CTX_EXECUTION_DATE=2022-09-14T03:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-09-14T03:00:00+00:00
[2022-09-18 20:16:00,531] {bigquery.py:680} INFO - Executing: 
           select id from githubarchive.day.20220914
        
[2022-09-18 20:16:00,869] {logging_mixin.py:103} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-09-18 20:16:01,519] {bigquery.py:1510} INFO - Inserting job airflow_1663532161384412_06d2bf8b43be9b79a3e89edf3b53e367
[2022-09-18 20:16:09,295] {taskinstance.py:1142} INFO - Marking task as SUCCESS. dag_id=bigquery_dag, task_id=query_big_query_table, execution_date=20220914T030000, start_date=20220918T201557, end_date=20220918T201609
[2022-09-18 20:16:11,515] {taskinstance.py:1195} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-09-18 20:16:11,852] {local_task_job.py:118} INFO - Task exited with return code 0
